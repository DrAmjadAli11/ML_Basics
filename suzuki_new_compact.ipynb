{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffdd4e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Head:\n",
      "         Date   Open   High    Low  Close   Volume\n",
      "0  2001-01-01  10.25  10.25  10.25  10.25   1000.0\n",
      "1  2001-01-02  10.25  11.50  10.70  11.30  10500.0\n",
      "2  2001-01-03  11.30  11.30  10.75  10.75   6500.0\n",
      "3  2001-01-04  10.75  11.30  11.25  11.25   3000.0\n",
      "4  2001-01-05  11.25  11.30  11.05  11.05   6500.0\n",
      "Original Dataset Tail:\n",
      "            Date    Open    High     Low   Close    Volume\n",
      "5247  2022-11-11  161.50  162.98  159.00  160.84   18222.0\n",
      "5248  2022-11-14  159.00  159.85  156.94  157.25   52296.0\n",
      "5249  2022-11-15  158.50  164.50  157.01  162.27  151394.0\n",
      "5250  2022-11-16  163.00  164.00  160.00  160.29   49327.0\n",
      "5251  2022-11-17  161.69  161.70  158.60  159.21   43334.0\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset suzuki_data.csv with dimensions: 5252x14\n",
    "# Stock price data of Pak Suzuki Motor Company Limited (Pakistan)\n",
    "# Stock prices daily data from 01 Jan 2001 to 17 Nov 2022 \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.read_csv('suzuki_data.csv')\n",
    "\n",
    "## Setting Hyperparameters\n",
    "MAX_epochs = 100       # Maximum number of epochs\n",
    "tol = 0.000001         # Tolerance\n",
    "alpha = 0.01           # Learning rate\n",
    "m = 100                # Number of instances in a minibatch\n",
    "window_size = 10\n",
    "train_test_ratio = 0.95\n",
    "\n",
    "# Setting Layers and Number of neurons in each layer\n",
    "ldim = [window_size, 20, 30, 10, 5, 1]\n",
    "L = len(ldim) - 1    # Number of layers (other than the input layer)\n",
    "\n",
    "ldim = np.array(ldim, dtype=int)\n",
    "np.random.seed(1)\n",
    "\n",
    "print(\"Original Dataset Head:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"Original Dataset Tail:\")\n",
    "print(df.tail(5))\n",
    "\n",
    "# Loading and making the data ready in desired format\n",
    "\n",
    "df = df.dropna()                # data cleaning as desired\n",
    "\n",
    "# Selecting the relevant column for training and prediction\n",
    "df_main = df['Close']\n",
    "df_main = np.array(df_main).reshape(-1,1)\n",
    "\n",
    "# normalizing the data between 0 to 1\n",
    "normalizing = MinMaxScaler(feature_range=(0,1))\n",
    "normalized_data = normalizing.fit_transform(df_main)\n",
    "\n",
    "## Spliting the time series data into train and test sets\n",
    "## Temporal order of the data is preserved\n",
    "\n",
    "train_size = int(len(normalized_data) * train_test_ratio)\n",
    "train_data = normalized_data[:train_size]\n",
    "test_data  = normalized_data[train_size:]\n",
    "\n",
    "# Function for creating datasets using Sliding Window Technique\n",
    "# This is a common technique for time series prediction using FNN\n",
    "# Slider window size is the time step\n",
    "\n",
    "def dataset_creator(window_size, data):\n",
    "    X, Y = [], []\n",
    "    for i in range(window_size, len(data)):\n",
    "        X.append(data[i-window_size:i, 0])\n",
    "        Y.append(data[i, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Creating the training and test sets\n",
    "X_train, Y_train = dataset_creator(window_size, train_data)\n",
    "X_test, Y_test = dataset_creator(window_size, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ddb2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Features ready with dimensions:  (10, 4979)\n",
      "Training Set Target ready with dimensions:  (1, 4979)\n",
      "Test Set Features ready with dimensions:  (10, 253)\n",
      "Test Set Target ready with dimensions:  (253,)\n"
     ]
    }
   ],
   "source": [
    "##============= Formatting the training set =============##\n",
    "\n",
    "N = X_train.shape[0]   # Total number of training instances\n",
    "gamma = int( np.ceil(N/m) )\n",
    "\n",
    "## converting Pandas objects to NumPy arrays and desired dimensions \n",
    "\n",
    "MM = X_train.T\n",
    "YY = Y_train.reshape(1, -1)\n",
    "\n",
    "print(\"Training Set Features ready with dimensions: \" , MM.shape)\n",
    "print(\"Training Set Target ready with dimensions: \" , YY.shape)\n",
    "\n",
    "##------------- Formatting the training set -------------##\n",
    "\n",
    "\n",
    "##============= Formatting the test set =============##\n",
    "\n",
    "Nt = X_test.shape[0]   # Total number of training instances\n",
    "\n",
    "## converting pandas objects to numpy arrays and desired dimensions \n",
    "Mt = X_test.T\n",
    "Yt = Y_test.T\n",
    "\n",
    "print(\"Test Set Features ready with dimensions: \" , Mt.shape )\n",
    "print(\"Test Set Target ready with dimensions: \" , Yt.shape )\n",
    "\n",
    "##------------- Formatting the test set -------------##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9842fc35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Defining activitation functions and their derivatives\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return np.where(Z <= 0, 0, 1)\n",
    "\n",
    "\n",
    "## Defining loss functions and their derivatives\n",
    "\n",
    "def quadratic_cost(P, Q):\n",
    "    return 0.5 * np.linalg.norm(P - Q) ** 2\n",
    "\n",
    "def quadratic_cost_derivative(P, Q):\n",
    "    return Q - P\n",
    "\n",
    "def binary_cross_entropy(P, Q):\n",
    "    err = -(np.dot(P,np.log(Q)) + np.dot((1-P),np.log(1-Q)))\n",
    "    #err = -(P*np.log(Q) + (1-P)*np.log(1-Q))\n",
    "    #err = np.squeeze(err)\n",
    "    return  err\n",
    "    \n",
    "def binary_cross_entropy_derivative(P, Q):\n",
    "    return (Q-P)/((1-Q) * Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc13405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the function that trains/optimizes model parameters\n",
    "\n",
    "def trainer(L, N, m, gamma, alpha, Nt, MAX_epochs, ldim, MM, YY):\n",
    "\n",
    "  ##  Initializing the parameters (weights and biases)\n",
    "    W = np.zeros( (L+1, np.max(ldim) , np.max(ldim)) )\n",
    "    B = np.zeros( (L+1, np.max(ldim) , 1))\n",
    "    dW = np.zeros( (L+1, np.max(ldim) , np.max(ldim)) )\n",
    "    dB = np.zeros( (L+1, np.max(ldim) , 1) )\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        W[ l , 0:ldim[l] , 0:ldim[l-1] ] = np.random.randn( \n",
    "                                        ldim[l], ldim[l-1] ) * 0.01\n",
    "        B[ l , 0:ldim[l] , : ] = np.zeros( (ldim[l], 1) )\n",
    "\n",
    "\n",
    "    ##  Creating auxiliary data structures\n",
    "\n",
    "    Y_hat = np.zeros( (L+1, np.max(ldim), m) )\n",
    "    Z     = np.zeros( (L+1, np.max(ldim), m) )\n",
    "    dZ    = np.zeros( (L+1, np.max(ldim), m) )\n",
    "\n",
    "    ## Loop over epochs\n",
    "\n",
    "    for k in range(1, MAX_epochs+1):\n",
    "    \n",
    "        # print('\\n========================= Epoch number: ', k)\n",
    "        # Making a random rearrangement of training instances\n",
    "        permut = np.random.permutation(N)\n",
    "        # permut = np.array([i for i in range(0,N)])\n",
    "        MM = MM[:, permut]\n",
    "        YY = YY[:, permut]\n",
    "\n",
    "        ## Loop over iterations / steps\n",
    "    \n",
    "        start_idx = -m\n",
    "        end_idx = 0\n",
    "        for g in range(1, gamma+1):\n",
    "           \n",
    "            start_idx = start_idx + m\n",
    "            end_idx = end_idx + m\n",
    "        \n",
    "            if end_idx>N:\n",
    "                end_idx = N\n",
    "        \n",
    "            mc = end_idx-start_idx\n",
    "        \n",
    "            # M (n0xm) is sampled from MM (n0xN)\n",
    "            M = MM[:, start_idx:end_idx]\n",
    "            # Y (1xm) is sampled from YY (1xN)\n",
    "            Y = YY[:, start_idx:end_idx]\n",
    "      \n",
    "            ## Carrying out computtaions of the Forward Pass\n",
    "            C = 0\n",
    "            Y_hat[0, 0:ldim[0], 0:mc] = M  \n",
    "        \n",
    "            for l in range(1, L + 1):\n",
    "                wx = W[l, 0:ldim[l], 0:ldim[l-1] ]\n",
    "                yold = Y_hat[l-1, 0:ldim[l-1], 0:mc]\n",
    "                bx = B[l, 0:ldim[l], :]\n",
    "            \n",
    "                zx = np.dot(wx, yold) + bx\n",
    "                Z[l, 0:ldim[l], 0:mc] = zx           \n",
    "            \n",
    "                #if l==L:\n",
    "                yx = sigmoid(zx)\n",
    "                #else:\n",
    "                #    yx = relu(zx)\n",
    "                \n",
    "                Y_hat[l, 0:ldim[l], 0:mc] = yx\n",
    "\n",
    "            ## computing cost function\n",
    "            for p in range(0,mc):\n",
    "                cp = quadratic_cost(Y[0:ldim[L],p], Y_hat[L,0:ldim[L], p])\n",
    "                #cp = binary_cross_entropy( Y[0:ldim[L], p],\n",
    "                                                #Y_hat[L, 0:ldim[L], p])\n",
    "                C += cp   \n",
    "            \n",
    "            C /= mc\n",
    "        \n",
    "        \n",
    "            ## Computing gradients through backpropagation\n",
    "            \n",
    "            dY_hat_L = quadratic_cost_derivative(Y[0:ldim[L], 0:mc],\n",
    "                                                 Y_hat[L, 0:ldim[L], 0:mc])\n",
    "            #dY_hat_L = binary_cross_entropy_derivative(Y[0:ldim[L], 0:mc],\n",
    "                                                #Y_hat[L, 0:ldim[L], 0:mc])\n",
    "            \n",
    "            dZ[L, 0:ldim[L], 0:mc] = dY_hat_L * sigmoid_derivative(\n",
    "                                                    Z[L, 0:ldim[L], 0:mc])\n",
    "            \n",
    "            for l in range(L-1, 0, -1):\n",
    "                wt = W[l+1, 0:ldim[l+1], 0:ldim[l]].T\n",
    "                dzt = dZ[l+1, 0:ldim[l+1],  0:mc]\n",
    "                zt = Z[l, 0:ldim[l], 0:mc]\n",
    "                #dZ[l,0:ldim[l],0:mc]= np.dot(wt,dzt)*relu_derivative(zt)\n",
    "                dZ[l,0:ldim[l],0:mc]= np.dot(wt,dzt)*sigmoid_derivative(zt)\n",
    "\n",
    "            for l in range(L, 0, -1):\n",
    "                dZl = dZ[l, 0:ldim[l], 0:mc]\n",
    "                Y_prev = Y_hat[l-1, 0:ldim[l-1], 0:mc]\n",
    "                dW[l, 0:ldim[l] , 0:ldim[l-1]] = (np.dot(dZl, Y_prev.T))/mc\n",
    "                dB[l, 0:ldim[l] , :] =  (np.sum(dZl, \n",
    "                                axis=1, keepdims=True))/mc\n",
    "            \n",
    "            ## Updating parameters using the Gradient Descent method\n",
    "            for l in range(L, 0, -1):\n",
    "                W[ l, 0:ldim[l] , 0:ldim[l-1] ] = W[ l, 0:ldim[l] ,\n",
    "                    0:ldim[l-1] ] - alpha*dW[ l, 0:ldim[l] , 0:ldim[l-1] ]\n",
    "                B[ l, 0:ldim[l] , : ] = B[ l, 0:ldim[l] , : ] \\\n",
    "                                            - alpha*dB[ l, 0:ldim[l] , : ]\n",
    "            \n",
    "        ## Loop terminated, over iterations / steps\n",
    "    \n",
    "    ## Loop terminated, over epochs\n",
    "   \n",
    "    print(f'After {k} epochs, the training Error (Cost function)= {C}')\n",
    "    \n",
    "    return W, B\n",
    "    # function trainer() ended\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537c1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the function that make prediction\n",
    "\n",
    "def predicting(L, ldim, W, B, Mt, Nt):\n",
    "\n",
    "    C = 0\n",
    "    Y_hat1 = np.zeros( (L+1, np.max(ldim), Nt) )\n",
    "    Y_hat1[0, 0:ldim[0], 0:Nt] = Mt\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "            wx = W[l, 0:ldim[l], 0:ldim[l-1] ]\n",
    "            yold = Y_hat1[l-1, 0:ldim[l-1], 0:Nt]\n",
    "            bx = B[l, 0:ldim[l], :]\n",
    "            \n",
    "            zx = np.dot(wx, yold) + bx\n",
    "            #Z1[l, 0:ldim[l], 0:Nt] = zx           \n",
    "            #if l==L:\n",
    "            yx = sigmoid(zx)\n",
    "            #else:\n",
    "            #    yx = relu(zx)\n",
    "                \n",
    "            Y_hat1[l, 0:ldim[l], 0:Nt] = yx\n",
    "            \n",
    "    return yx\n",
    "    # function predicting() ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dca29087",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the function for calculating MAE and RMSE\n",
    "\n",
    "def  reg_errors(Y_test, Y_predict_normalized):\n",
    "\n",
    "    Y_predict = normalizing.inverse_transform(Y_predict_normalized)\n",
    "    Y_test1 = normalizing.inverse_transform(Y_test.reshape(-1,1))\n",
    "\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    mae = mean_absolute_error(Y_test1,Y_predict.T)\n",
    "    mse = mean_squared_error(Y_test1,Y_predict.T)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(\"MAE in the prediction for the test data:\", mae)\n",
    "    print(\"RMSE in the prediction for the test data: \", rmse)\n",
    "    \n",
    "    # function reg_errors() ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e585b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Training epochs started ==========\n",
      "\n",
      "After 100 epochs, the training Error (Cost function)= 0.015396965552653855\n",
      "\n",
      "========== Training epochs completed ==========\n",
      "\n",
      "\n",
      "\n",
      "~~~~~ Forward Pass with the test data started ~~~~~\n",
      "\n",
      "================== Prediction Completed ==================\n",
      "\n",
      "======================= Test Errors =======================\n",
      "MAE in the prediction for the test data: 30.344541839324346\n",
      "RMSE in the prediction for the test data:  37.36703073558258\n"
     ]
    }
   ],
   "source": [
    "print('\\n========== Training epochs started ==========\\n')\n",
    "W, B = trainer(L, N, m, gamma, alpha, Nt, MAX_epochs, ldim, MM, YY)\n",
    "print('\\n========== Training epochs completed ==========')\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "## using the optimized parameters for making predictions\n",
    "## The Forward Pass with the test data\n",
    "print(\"\\n~~~~~ Forward Pass with the test data started ~~~~~\") \n",
    "Y_predict_normalized = predicting(L, ldim, W, B, Mt, Nt)\n",
    "print(\"\\n================== Prediction Completed ==================\") \n",
    "    \n",
    "    \n",
    "print(\"\\n======================= Test Errors =======================\")\n",
    "reg_errors(Y_test, Y_predict_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4cf55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
